{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import makedirs\n",
    "from os.path import dirname\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/preprocessed/preprocessed.csv')\n",
    "df_orig = df.copy()\n",
    "df['StartTime'] = pd.to_datetime(df['StartTime'])\n",
    "df['epoch'] = ((df['StartTime'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1ms')) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = df.copy().sort_values('StartTime', ignore_index=True)\n",
    "\n",
    "def create_fwd_bwd_col(f_df, col, name):\n",
    "    df = f_df.copy()\n",
    "    \n",
    "    #Forward direction\n",
    "    fwd_name = f'{name}_fwd'\n",
    "    df[fwd_name] = df[col]\n",
    "    df.loc[df.is_fwd == 0, fwd_name] = np.NaN\n",
    "\n",
    "    #Backword direction\n",
    "    bwd_name = f'{name}_bwd'\n",
    "    df[bwd_name] = df[col]\n",
    "    df.loc[df.is_fwd == 1, bwd_name] = np.NaN\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_status(index, total, percentage=5):\n",
    "    threshold = int(total*(percentage/100))\n",
    "    if index % threshold == 0:\n",
    "        completion = index/total * 100\n",
    "        print(f'TASK: {completion}% Completed')\n",
    "\n",
    "def addr_prefix_name(addr):\n",
    "    if addr == 'SrcAddr':\n",
    "        return 'S'\n",
    "    if addr == 'DstAddr':\n",
    "        return 'D'\n",
    "    return ''\n",
    "\n",
    "def convert_to_int_64(df, columns):\n",
    "    copy = df.copy()\n",
    "    for col in columns:\n",
    "        copy[col] = copy[col].astype('int64')\n",
    "    return copy\n",
    "\n",
    "def build_tot_flows_time_window(f_df, time):\n",
    "    '''\n",
    "        Total Flow in forward and backward 10 min\n",
    "        10min = 10T\n",
    "    '''\n",
    "    df = f_df.copy()\n",
    "    df_with_bwd = df.copy()\n",
    "    df_with_bwd['is_bwd'] = df_with_bwd['is_fwd'].replace({0:1, 1:0})\n",
    "    df[f'TotFlowFwd_{time}'] = df[['StartTime', 'is_fwd']].rolling(time, on='StartTime').sum()['is_fwd']\n",
    "    df[f'TotFlowBwd_{time}'] = df_with_bwd[['StartTime', 'is_bwd']].rolling(time, on='StartTime').sum()['is_bwd']\n",
    "    return convert_to_int_64(df, [f'TotFlowFwd_{time}', f'TotFlowBwd_{time}'])\n",
    "\n",
    "def build_time_features(f_df, col_name, short_name, time):\n",
    "    '''Time should be pandas rolling period'''\n",
    "    df = f_df.copy()\n",
    "    \n",
    "    #Require a df with exta attributes b/c of the rolling.\n",
    "    #See create_fwd_bwd_col\n",
    "    #Forward direction and backward of the given column.\n",
    "    df_rolling = create_fwd_bwd_col(df, col_name, short_name)\n",
    "\n",
    "    fwd_col = short_name + '_fwd'\n",
    "    bwd_col = short_name + '_bwd'\n",
    "    \n",
    "    time_window = df_rolling[['StartTime', fwd_col, bwd_col]].rolling(time, on='StartTime')\n",
    "\n",
    "    #Total Size in Forward and Backward based on time\n",
    "    sums = time_window.sum()\n",
    "    sum_fwd_name, sum_bwd_name = f'{short_name}SumFwd_{time}', f'{short_name}SumBwd_{time}'\n",
    "    df[sum_fwd_name] = sums[fwd_col]\n",
    "    df[sum_bwd_name] = sums[bwd_col]\n",
    "\n",
    "    #Min Size in Forward and Backward based on time\n",
    "    mins = time_window.min()\n",
    "    min_fwd_name, min_bwd_name = f'{short_name}MinFwd_{time}', f'{short_name}MinBwd_{time}'\n",
    "    df[min_fwd_name] = mins[fwd_col]\n",
    "    df[min_bwd_name] = mins[bwd_col]\n",
    "\n",
    "    #Max Size in Forward and Backward based on time\n",
    "    maxs = time_window.max()\n",
    "    max_fwd_name, max_bwd_name = f'{short_name}MaxFwd_{time}', f'{short_name}MaxBwd_{time}'\n",
    "    df[max_fwd_name] = maxs[fwd_col]\n",
    "    df[max_bwd_name] = maxs[bwd_col]\n",
    "\n",
    "    #Mean Size in Forward and Backward based on time\n",
    "    means = time_window.mean()\n",
    "    mean_fwd_name, mean_bwd_name = f'{short_name}MeanFwd_{time}', f'{short_name}MeanBwd_{time}'\n",
    "    df[mean_fwd_name] = means[fwd_col]\n",
    "    df[mean_bwd_name] = means[bwd_col]\n",
    "\n",
    "    #Standard Deviation Size in Forward and Backward based on time\n",
    "    stds = time_window.std()\n",
    "    std_fwd_name, std_bwd_name = f'{short_name}StdFwd_{time}', f'{short_name}StdBwd_{time}'\n",
    "    df[std_fwd_name] = stds[fwd_col]\n",
    "    df[std_bwd_name] = stds[bwd_col]\n",
    "    \n",
    "    #Fill all columns with np.NaN with zero\n",
    "    df[[sum_fwd_name,\n",
    "        sum_bwd_name,\n",
    "        min_fwd_name,\n",
    "        min_bwd_name,\n",
    "        max_fwd_name,\n",
    "        max_bwd_name,\n",
    "        mean_fwd_name,\n",
    "        mean_bwd_name,\n",
    "        std_fwd_name,\n",
    "        std_bwd_name\n",
    "       ]] = df[[sum_fwd_name,\n",
    "                sum_bwd_name,\n",
    "                min_fwd_name,\n",
    "                min_bwd_name,\n",
    "                max_fwd_name,\n",
    "                max_bwd_name,\n",
    "                mean_fwd_name,\n",
    "                mean_bwd_name,\n",
    "                std_fwd_name,\n",
    "                std_bwd_name\n",
    "               ]].fillna(0)\n",
    "    \n",
    "     #All columns that can be int, convert to int.\n",
    "    return convert_to_int_64(df, [sum_fwd_name, sum_bwd_name, min_fwd_name, min_bwd_name, max_fwd_name, max_bwd_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = build_tot_flows_time_window(feature_df, '10T')\n",
    "feature_df = build_time_features(feature_df, 'TotBytes', 'TotB', '10T')\n",
    "feature_df = build_time_features(feature_df, 'TotPkts', 'TotPkt', '10T')\n",
    "feature_df = build_time_features(feature_df, 'SrcBytes', 'SrcB', '10T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pkts_bytes_x_window(f_df, num, addr):\n",
    "    df = f_df[[addr, 'is_fwd', 'TotPkts', 'TotBytes', 'SrcBytes', 'epoch']].copy()\n",
    "    total_length = len(df.index)\n",
    "    window = []\n",
    "    prefix = addr_prefix_name(addr)\n",
    "\n",
    "    #Total Flow\n",
    "    tot_flow_fwd_l = []\n",
    "    tot_flow_bwd_l = []\n",
    "    \n",
    "    #Total Bytes Forward\n",
    "    tot_b_sum_fwd_l = []\n",
    "    tot_b_min_fwd_l = []\n",
    "    tot_b_max_fwd_l = []\n",
    "    tot_b_mean_fwd_l = []\n",
    "    tot_b_std_fwd_l = []\n",
    "    \n",
    "    #Total Bytes Backward\n",
    "    tot_b_sum_bwd_l = []\n",
    "    tot_b_min_bwd_l = []\n",
    "    tot_b_max_bwd_l = []\n",
    "    tot_b_mean_bwd_l = []\n",
    "    tot_b_std_bwd_l = []\n",
    "    \n",
    "    #Total Packets Forward\n",
    "    tot_pkt_sum_fwd_l = []\n",
    "    tot_pkt_min_fwd_l = []\n",
    "    tot_pkt_max_fwd_l = []\n",
    "    tot_pkt_mean_fwd_l = []\n",
    "    tot_pkt_std_fwd_l = []\n",
    "\n",
    "    #Total Packets Backward\n",
    "    tot_pkt_sum_bwd_l = []\n",
    "    tot_pkt_min_bwd_l = []\n",
    "    tot_pkt_max_bwd_l = []\n",
    "    tot_pkt_mean_bwd_l = []\n",
    "    tot_pkt_std_bwd_l = []\n",
    "    \n",
    "    #Total Src Bytes Forward\n",
    "    src_b_sum_fwd_l = []\n",
    "    src_b_min_fwd_l = []\n",
    "    src_b_max_fwd_l = []\n",
    "    src_b_mean_fwd_l = []\n",
    "    src_b_std_fwd_l = []\n",
    "    \n",
    "    #Total Src Bytes Backward\n",
    "    src_b_sum_bwd_l = []\n",
    "    src_b_min_bwd_l = []\n",
    "    src_b_max_bwd_l = []\n",
    "    src_b_mean_bwd_l = []\n",
    "    src_b_std_bwd_l = []\n",
    "    \n",
    "    #Time Between Flows\n",
    "    time_flow_fwd_sec_l = []\n",
    "    time_flow_bwd_sec_l = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        print_status(index, total_length)\n",
    "        \n",
    "        #Add row to window\n",
    "        if len(window) == num:\n",
    "            window.pop(0)\n",
    "        window.append(row)\n",
    "        \n",
    "        #Get all rows with the same address and in forward or backward directions.\n",
    "        current_row_addr = row[addr]\n",
    "        fwd_item_list = [ item for item in window if item[addr] == current_row_addr and item.is_fwd]\n",
    "        bwd_item_list = [ item for item in window if item[addr] == current_row_addr and not item.is_fwd]\n",
    "        \n",
    "        #Total Flow\n",
    "        tot_flow_fwd_l.append(len(fwd_item_list))\n",
    "        tot_flow_bwd_l.append(len(bwd_item_list))\n",
    "        \n",
    "        #Total Bytes\n",
    "        tot_b_fwd_l = np.array([ item.TotBytes for item in fwd_item_list])\n",
    "        tot_b_sum_fwd_l.append(tot_b_fwd_l.sum() if len(tot_b_fwd_l) != 0 else 0)\n",
    "        tot_b_min_fwd_l.append(tot_b_fwd_l.min() if len(tot_b_fwd_l) != 0 else 0)\n",
    "        tot_b_max_fwd_l.append(tot_b_fwd_l.max() if len(tot_b_fwd_l) != 0 else 0)\n",
    "        tot_b_mean_fwd_l.append(tot_b_fwd_l.mean() if len(tot_b_fwd_l) != 0 else 0)\n",
    "        tot_b_std_fwd_l.append(tot_b_fwd_l.std() if len(tot_b_fwd_l) != 0 else 0)\n",
    "        \n",
    "        tot_b_bwd_l = np.array([ item.TotBytes for item in bwd_item_list])\n",
    "        tot_b_sum_bwd_l.append(tot_b_bwd_l.sum() if len(tot_b_bwd_l) != 0 else 0)\n",
    "        tot_b_min_bwd_l.append(tot_b_bwd_l.min() if len(tot_b_bwd_l) != 0 else 0)\n",
    "        tot_b_max_bwd_l.append(tot_b_bwd_l.max() if len(tot_b_bwd_l) != 0 else 0)\n",
    "        tot_b_mean_bwd_l.append(tot_b_bwd_l.mean() if len(tot_b_bwd_l) != 0 else 0)\n",
    "        tot_b_std_bwd_l.append(tot_b_bwd_l.std() if len(tot_b_bwd_l) != 0 else 0)\n",
    "        \n",
    "        #Total Packets\n",
    "        tot_pkts_fwd_l = np.array([ item.TotPkts for item in fwd_item_list])\n",
    "        tot_pkt_sum_fwd_l.append(tot_pkts_fwd_l.sum() if len(tot_pkts_fwd_l) != 0 else 0)\n",
    "        tot_pkt_min_fwd_l.append(tot_pkts_fwd_l.min() if len(tot_pkts_fwd_l) != 0 else 0)\n",
    "        tot_pkt_max_fwd_l.append(tot_pkts_fwd_l.max() if len(tot_pkts_fwd_l) != 0 else 0)\n",
    "        tot_pkt_mean_fwd_l.append(tot_pkts_fwd_l.mean() if len(tot_pkts_fwd_l) != 0 else 0)\n",
    "        tot_pkt_std_fwd_l.append(tot_pkts_fwd_l.std() if len(tot_pkts_fwd_l) != 0 else 0)\n",
    "        \n",
    "        tot_pkts_bwd_l = np.array([ item.TotPkts for item in bwd_item_list])\n",
    "        tot_pkt_sum_bwd_l.append(tot_pkts_bwd_l.sum() if len(tot_pkts_bwd_l) != 0 else 0)\n",
    "        tot_pkt_min_bwd_l.append(tot_pkts_bwd_l.min() if len(tot_pkts_bwd_l) != 0 else 0)\n",
    "        tot_pkt_max_bwd_l.append(tot_pkts_bwd_l.max() if len(tot_pkts_bwd_l) != 0 else 0)\n",
    "        tot_pkt_mean_bwd_l.append(tot_pkts_bwd_l.mean() if len(tot_pkts_bwd_l) != 0 else 0)\n",
    "        tot_pkt_std_bwd_l.append(tot_pkts_bwd_l.std() if len(tot_pkts_bwd_l) != 0 else 0)\n",
    "        \n",
    "        #Total Source Bytes\n",
    "        src_b_fwd_l = np.array([ item.SrcBytes for item in fwd_item_list])\n",
    "        src_b_sum_fwd_l.append(src_b_fwd_l.sum() if len(src_b_fwd_l) != 0 else 0)\n",
    "        src_b_min_fwd_l.append(src_b_fwd_l.min() if len(src_b_fwd_l) != 0 else 0)\n",
    "        src_b_max_fwd_l.append(src_b_fwd_l.max() if len(src_b_fwd_l) != 0 else 0)\n",
    "        src_b_mean_fwd_l.append(src_b_fwd_l.mean() if len(src_b_fwd_l) != 0 else 0)\n",
    "        src_b_std_fwd_l.append(src_b_fwd_l.std() if len(src_b_fwd_l) != 0 else 0)\n",
    "        \n",
    "        src_b_bwd_l = np.array([ item.SrcBytes for item in bwd_item_list])\n",
    "        src_b_sum_bwd_l.append(src_b_bwd_l.sum() if len(src_b_bwd_l) != 0 else 0)\n",
    "        src_b_min_bwd_l.append(src_b_bwd_l.min() if len(src_b_bwd_l) != 0 else 0)\n",
    "        src_b_max_bwd_l.append(src_b_bwd_l.max() if len(src_b_bwd_l) != 0 else 0)\n",
    "        src_b_mean_bwd_l.append(src_b_bwd_l.mean() if len(src_b_bwd_l) != 0 else 0)\n",
    "        src_b_std_bwd_l.append(src_b_bwd_l.std() if len(src_b_bwd_l) != 0 else 0)\n",
    "        \n",
    "        #Time Between Flows\n",
    "        if len(fwd_item_list) < 2:\n",
    "            time_flow_fwd_sec_l.append(0)\n",
    "        else:\n",
    "            time_flow_fwd_sec_l.append(fwd_item_list[-1].epoch - fwd_item_list[-2].epoch)\n",
    "\n",
    "        if len(bwd_item_list) < 2:\n",
    "            time_flow_bwd_sec_l.append(0)\n",
    "        else:\n",
    "            time_flow_bwd_sec_l.append(bwd_item_list[-1].epoch - bwd_item_list[-2].epoch)\n",
    "\n",
    "    #Build Data Frame\n",
    "    new_features_df = pd.DataFrame(data={\n",
    "        f'{prefix}TotFlowFwdN_{num}': tot_flow_fwd_l,\n",
    "        f'{prefix}TotFlowBwdN_{num}': tot_flow_bwd_l,\n",
    "        f'{prefix}TotBSumFwdN_{num}': tot_b_sum_fwd_l,\n",
    "        f'{prefix}TotBMinFwdN_{num}': tot_b_min_fwd_l,\n",
    "        f'{prefix}TotBMaxFwdN_{num}': tot_b_max_fwd_l,\n",
    "        f'{prefix}TotBMeanFwdN_{num}': tot_b_mean_fwd_l,\n",
    "        f'{prefix}TotBStdFwdN_{num}': tot_b_std_fwd_l,\n",
    "        f'{prefix}TotBSumBwdN_{num}': tot_b_sum_bwd_l,\n",
    "        f'{prefix}TotBMinBwdN_{num}': tot_b_min_bwd_l,\n",
    "        f'{prefix}TotBMaxBwdN_{num}': tot_b_max_bwd_l,\n",
    "        f'{prefix}TotBMeanBwdN_{num}': tot_b_mean_bwd_l,\n",
    "        f'{prefix}TotBStdBwdN_{num}': tot_b_std_bwd_l,\n",
    "        f'{prefix}TotPktSumFwdN_{num}': tot_pkt_sum_fwd_l,\n",
    "        f'{prefix}TotPktMinFwdN_{num}': tot_pkt_min_fwd_l,\n",
    "        f'{prefix}TotPktMaxFwdN_{num}': tot_pkt_max_fwd_l,\n",
    "        f'{prefix}TotPktMeanFwdN_{num}': tot_pkt_mean_fwd_l,\n",
    "        f'{prefix}TotPktStdFwdN_{num}': tot_pkt_std_fwd_l,\n",
    "        f'{prefix}TotPktSumBwdN_{num}': tot_pkt_sum_bwd_l,\n",
    "        f'{prefix}TotPktMinBwdN_{num}': tot_pkt_min_bwd_l,\n",
    "        f'{prefix}TotPktMaxBwdN_{num}': tot_pkt_max_bwd_l,\n",
    "        f'{prefix}TotPktMeanBwdN_{num}': tot_pkt_mean_bwd_l,\n",
    "        f'{prefix}TotPktStdBwdN_{num}': tot_pkt_std_bwd_l,\n",
    "        f'{prefix}SrcBSumFwdN_{num}': src_b_sum_fwd_l,\n",
    "        f'{prefix}SrcBMinFwdN_{num}': src_b_min_fwd_l,\n",
    "        f'{prefix}SrcBMaxFwdN_{num}': src_b_max_fwd_l,\n",
    "        f'{prefix}SrcBMeanFwdN_{num}': src_b_mean_fwd_l,\n",
    "        f'{prefix}SrcBStdFwdN_{num}': src_b_std_fwd_l,\n",
    "        f'{prefix}SrcBSumBwdN_{num}': src_b_sum_bwd_l,\n",
    "        f'{prefix}SrcBMinBwdN_{num}': src_b_min_bwd_l,\n",
    "        f'{prefix}SrcBMaxBwdN_{num}': src_b_max_bwd_l,\n",
    "        f'{prefix}SrcBMeanBwdN_{num}': src_b_mean_bwd_l,\n",
    "        f'{prefix}SrcBStdBwdN_{num}': src_b_std_bwd_l,\n",
    "        f'{prefix}Time2FlowFwdN_{num}': time_flow_fwd_sec_l,\n",
    "        f'{prefix}Time2FlowBwdN_{num}': time_flow_bwd_sec_l\n",
    "    })\n",
    "    return pd.concat([f_df, new_features_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_trim_dict_value_list(key, dictionary, size):\n",
    "    if key in dictionary:\n",
    "        while len(dictionary[key]) >= size:\n",
    "            dictionary[key].pop(0)\n",
    "    else:\n",
    "        dictionary[key] = []\n",
    "\n",
    "def helper_calc_flow_diff(l, window_range_sec):\n",
    "    if len(l) < 2 or (l[-1] - l[-2]) > window_range_sec:\n",
    "        return 0\n",
    "    return l[-1] - l[-2]\n",
    "\n",
    "def build_time_bet_2_flow_time_window(f_df, minutes):\n",
    "    df = f_df[['SrcAddr', 'DstAddr', 'is_fwd', 'epoch']].copy()\n",
    "    total = len(df.index)\n",
    "    window_range_sec = minutes * 60\n",
    "    \n",
    "    src_fwd_dict = {}\n",
    "    src_bwd_dict = {}\n",
    "    dst_fwd_dict = {}\n",
    "    dst_bwd_dict = {}\n",
    "    \n",
    "    #Time Between Flows\n",
    "    src_fwd_list = []\n",
    "    src_bwd_list = []\n",
    "    \n",
    "    dst_fwd_list = []\n",
    "    dst_bwd_list = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        print_status(index, total)\n",
    "        if row.is_fwd:\n",
    "            helper_trim_dict_value_list(row.SrcAddr, src_fwd_dict, 2)\n",
    "            helper_trim_dict_value_list(row.DstAddr, dst_fwd_dict, 2)\n",
    "            \n",
    "            src_fwd_dict[row.SrcAddr].append(row.epoch)\n",
    "            dst_fwd_dict[row.DstAddr].append(row.epoch)\n",
    "            \n",
    "            src_bwd_list.append(0)\n",
    "            dst_bwd_list.append(0)\n",
    "            \n",
    "            #Forward Src\n",
    "            src_fwd_list.append(helper_calc_flow_diff(src_fwd_dict[row.SrcAddr], window_range_sec))\n",
    "            #Forward Dst\n",
    "            dst_fwd_list.append(helper_calc_flow_diff(dst_fwd_dict[row.DstAddr], window_range_sec))\n",
    "        else:\n",
    "            helper_trim_dict_value_list(row.SrcAddr, src_bwd_dict, 2)\n",
    "            helper_trim_dict_value_list(row.DstAddr, dst_bwd_dict, 2)\n",
    "            \n",
    "            src_bwd_dict[row.SrcAddr].append(row.epoch)\n",
    "            dst_bwd_dict[row.DstAddr].append(row.epoch)\n",
    "            \n",
    "            src_fwd_list.append(0)\n",
    "            dst_fwd_list.append(0)\n",
    "            \n",
    "            #Backward Src\n",
    "            src_bwd_list.append(helper_calc_flow_diff(src_bwd_dict[row.SrcAddr], window_range_sec))\n",
    "            #Backward Dst\n",
    "            dst_bwd_list.append(helper_calc_flow_diff(dst_bwd_dict[row.DstAddr], window_range_sec))\n",
    "    #Build Data Frame\n",
    "    new_features_df = pd.DataFrame(data={\n",
    "        f'STime2FlowFwd_{minutes}T': src_fwd_list,\n",
    "        f'STime2FlowBwd_{minutes}T': src_bwd_list,\n",
    "        f'DTime2FlowFwd_{minutes}T': dst_fwd_list,\n",
    "        f'DTime2FlowBwd_{minutes}T': dst_bwd_list\n",
    "    })\n",
    "    return pd.concat([f_df, new_features_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TASK: 0.0% Completed\n",
      "TASK: 4.999969252042762% Completed\n",
      "TASK: 9.999938504085524% Completed\n",
      "TASK: 14.999907756128286% Completed\n",
      "TASK: 19.999877008171048% Completed\n",
      "TASK: 24.99984626021381% Completed\n",
      "TASK: 29.99981551225657% Completed\n",
      "TASK: 34.999784764299335% Completed\n",
      "TASK: 39.999754016342095% Completed\n",
      "TASK: 44.99972326838486% Completed\n",
      "TASK: 49.99969252042762% Completed\n",
      "TASK: 54.99966177247039% Completed\n",
      "TASK: 59.99963102451314% Completed\n",
      "TASK: 64.99960027655591% Completed\n",
      "TASK: 69.99956952859867% Completed\n",
      "TASK: 74.99953878064144% Completed\n",
      "TASK: 79.99950803268419% Completed\n",
      "TASK: 84.99947728472696% Completed\n",
      "TASK: 89.99944653676972% Completed\n",
      "TASK: 94.99941578881248% Completed\n",
      "TASK: 99.99938504085524% Completed\n"
     ]
    }
   ],
   "source": [
    "feature_df = build_pkts_bytes_x_window(feature_df, 5, 'SrcAddr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TASK: 0.0% Completed\n",
      "TASK: 4.999969252042762% Completed\n",
      "TASK: 9.999938504085524% Completed\n",
      "TASK: 14.999907756128286% Completed\n",
      "TASK: 19.999877008171048% Completed\n",
      "TASK: 24.99984626021381% Completed\n",
      "TASK: 29.99981551225657% Completed\n",
      "TASK: 34.999784764299335% Completed\n",
      "TASK: 39.999754016342095% Completed\n",
      "TASK: 44.99972326838486% Completed\n",
      "TASK: 49.99969252042762% Completed\n",
      "TASK: 54.99966177247039% Completed\n",
      "TASK: 59.99963102451314% Completed\n",
      "TASK: 64.99960027655591% Completed\n",
      "TASK: 69.99956952859867% Completed\n",
      "TASK: 74.99953878064144% Completed\n",
      "TASK: 79.99950803268419% Completed\n",
      "TASK: 84.99947728472696% Completed\n",
      "TASK: 89.99944653676972% Completed\n",
      "TASK: 94.99941578881248% Completed\n",
      "TASK: 99.99938504085524% Completed\n"
     ]
    }
   ],
   "source": [
    "feature_df = build_pkts_bytes_x_window(feature_df, 5, 'DstAddr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TASK: 0.0% Completed\n",
      "TASK: 4.999969252042762% Completed\n",
      "TASK: 9.999938504085524% Completed\n",
      "TASK: 14.999907756128286% Completed\n",
      "TASK: 19.999877008171048% Completed\n",
      "TASK: 24.99984626021381% Completed\n",
      "TASK: 29.99981551225657% Completed\n",
      "TASK: 34.999784764299335% Completed\n",
      "TASK: 39.999754016342095% Completed\n",
      "TASK: 44.99972326838486% Completed\n",
      "TASK: 49.99969252042762% Completed\n",
      "TASK: 54.99966177247039% Completed\n",
      "TASK: 59.99963102451314% Completed\n",
      "TASK: 64.99960027655591% Completed\n",
      "TASK: 69.99956952859867% Completed\n",
      "TASK: 74.99953878064144% Completed\n",
      "TASK: 79.99950803268419% Completed\n",
      "TASK: 84.99947728472696% Completed\n",
      "TASK: 89.99944653676972% Completed\n",
      "TASK: 94.99941578881248% Completed\n",
      "TASK: 99.99938504085524% Completed\n"
     ]
    }
   ],
   "source": [
    "feature_df = build_time_bet_2_flow_time_window(feature_df, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write Sample to CSV\n",
    "makedirs(dirname('../data/processed/'), exist_ok=True)\n",
    "feature_df.head(10).to_csv('../data/processed/sample_processed.csv', index=False) #Includes epoch in sample file to help validate.\n",
    "feature_df = feature_df.drop(columns=['epoch']) #Remove as it is not part of the feature list\n",
    "#Write Raw and Features to CSV file.\n",
    "feature_df.to_csv('../data/processed/processed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('RDM-env': venv)",
   "language": "python",
   "name": "python38164bitrdmenvvenv4b617ac6a3c04225a7e20584d0a1c7e8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
