{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "df = pd.read_csv('../data/preprocessed/preprocessed.csv')\n",
    "df_orig = df.copy()\n",
    "df['StartTime'] = pd.to_datetime(df['StartTime'])\n",
    "df['epoch'] = (df['StartTime'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_map = {\n",
    "    'Dir':['   ->','   ?>','  <?>','  <->','  <-','  <?'],\n",
    "    'dir_map': [1, 2, 3, 4, 5, 6]\n",
    "}\n",
    "dirs_df = pd.DataFrame(data=dir_map)\n",
    "\n",
    "feature_df = df.copy()\n",
    "feature_df = feature_df.merge(dirs_df,how='left', on=['Dir'])\n",
    "feature_df['is_forward'] = feature_df.dir_map\n",
    "feature_df['is_backward'] = feature_df.dir_map\n",
    "feature_df.loc[feature_df.dir_map != 1, 'is_forward'] = 0\n",
    "feature_df.loc[feature_df.dir_map != 5, 'is_backward'] = 0\n",
    "feature_df.loc[feature_df.dir_map == 5, 'is_backward'] = 1\n",
    "\n",
    "feature_df['totbytes_f'] = feature_df.TotBytes\n",
    "feature_df.loc[feature_df.is_forward == 0, 'totbytes_f'] = np.NaN\n",
    "\n",
    "feature_df['totbytes_b'] = feature_df.TotBytes\n",
    "feature_df.loc[feature_df.is_backward == 0, 'totbytes_b'] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 0\n",
    "file_sep_list = []\n",
    "\n",
    "for index, value in feature_df.epoch.items():\n",
    "    if index == 0:\n",
    "        max_epoch = value\n",
    "        file_sep_list.append([index, value])\n",
    "        continue\n",
    "    if value > max_epoch:\n",
    "        max_epoch = value\n",
    "    if value < max_epoch and value < 4000:\n",
    "        file_sep_list.append([index, value])\n",
    "        max_epoch = value\n",
    "\n",
    "all_df = []\n",
    "for i in range (0, len(file_sep_list)):\n",
    "    if i == 0:\n",
    "        all_df.append(feature_df.iloc[:file_sep_list[i+1][0], :len(feature_df.columns)])\n",
    "    elif i + 1 == len(file_sep_list):\n",
    "        all_df.append(feature_df.iloc[file_sep_list[i][0]:len(feature_df.index)-1, :len(feature_df.columns)])\n",
    "    \n",
    "    else:\n",
    "        all_df.append(feature_df.iloc[file_sep_list[i][0]:file_sep_list[i+1][0], :len(feature_df.columns)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_frames = []\n",
    "for frames in all_df:\n",
    "    split_frames.append(frames.reset_index(drop=True).sort_values('StartTime'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total Flow in forward and backward 10 min\n",
    "for frames in split_frames:\n",
    "    frames['total_flow_f_time_10'] = frames[['StartTime', 'is_forward']].rolling('10T', on='StartTime').sum()['is_forward']\n",
    "    frames['total_flow_b_time_10'] = frames[['StartTime', 'is_backward']].rolling('10T', on='StartTime').sum()['is_backward']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frames in split_frames:\n",
    "    forward_min_10_rolling_tot_bytes = frames[['StartTime', 'totbytes_f']].rolling('10T', on='StartTime')\n",
    "    back_min_10_rolling_tot_bytes = frames[['StartTime', 'totbytes_b']].rolling('10T', on='StartTime')\n",
    "\n",
    "    #Total Size in Forward and Backward 10 minutes\n",
    "    frames['sum_size_f_time_10'] = forward_min_10_rolling_tot_bytes.sum()['totbytes_f']\n",
    "    frames['sum_size_b_time_10'] = back_min_10_rolling_tot_bytes.sum()['totbytes_b']\n",
    "\n",
    "    #Min Size in Forward and Backward 10 minutes\n",
    "    frames['min_size_f_time_10'] = forward_min_10_rolling_tot_bytes.min()['totbytes_f']\n",
    "    frames['min_size_b_time_10'] = back_min_10_rolling_tot_bytes.min()['totbytes_b']\n",
    "\n",
    "    #Max Size in Forward and Backward 10 minutes\n",
    "    frames['max_size_f_time_10'] = forward_min_10_rolling_tot_bytes.max()['totbytes_f']\n",
    "    frames['max_size_b_time_10'] = back_min_10_rolling_tot_bytes.max()['totbytes_b']\n",
    "\n",
    "    #Mean Size in Forward and Backward 10 minutes\n",
    "    frames['mean_size_f_time_10'] = forward_min_10_rolling_tot_bytes.mean()['totbytes_f']\n",
    "    frames['mean_size_b_time_10'] = back_min_10_rolling_tot_bytes.mean()['totbytes_b']\n",
    "\n",
    "    #Standard Deviation Size in Forward and Backward 10 minutes\n",
    "    frames['std_size_f_time_10'] = forward_min_10_rolling_tot_bytes.std()['totbytes_f']\n",
    "    frames['std_size_b_time_10'] = back_min_10_rolling_tot_bytes.std()['totbytes_b']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def src_flow_total(window, f_or_b):\n",
    "    current_row_src = window[-1]['SrcAddr']\n",
    "    if f_or_b == 'is_forward':\n",
    "        return np.array([ row['is_forward'] for row in window if row['SrcAddr'] == current_row_src and row['is_forward'] == 1])\n",
    "    else:\n",
    "        return np.array([ row['is_backward'] for row in window if row['SrcAddr'] == current_row_src and row['is_backward'] == 1])\n",
    "\n",
    "def sum_flow_window(row, window, f_or_b, num):\n",
    "    if len(window) == num:\n",
    "        window.pop(0)\n",
    "    window.append(row)\n",
    "    arr = src_flow_total(window, f_or_b)\n",
    "    return arr.sum() if len(arr) != 0 else 0.0\n",
    "\n",
    "def src_windows(window, f_or_b):\n",
    "    current_row_src = window[-1]['SrcAddr']\n",
    "    if f_or_b == 'is_forward':\n",
    "        return np.array([ row['totbytes_f'] for row in window if row['SrcAddr'] == current_row_src and row['is_forward'] == 1])\n",
    "    else:\n",
    "        return np.array([ row['totbytes_b'] for row in window if row['SrcAddr'] == current_row_src and row['is_backward'] == 1])\n",
    "\n",
    "def sum_bytes_window(row, window, f_or_b, num):\n",
    "    if len(window) == num:\n",
    "        window.pop(0)\n",
    "    window.append(row)\n",
    "    arr = src_windows(window, f_or_b)\n",
    "    return arr.sum() if len(arr) != 0 else np.NaN\n",
    "\n",
    "def min_bytes_window(row, window, f_or_b, num):\n",
    "    if len(window) == num:\n",
    "        window.pop(0)\n",
    "    window.append(row)\n",
    "    arr = src_windows(window, f_or_b)\n",
    "    return arr.min() if len(arr) != 0 else np.NaN\n",
    "\n",
    "def max_bytes_window(row, window, f_or_b, num):\n",
    "    if len(window) == num:\n",
    "        window.pop(0)\n",
    "    window.append(row)\n",
    "    arr = src_windows(window, f_or_b)\n",
    "    return arr.max() if len(arr) != 0 else np.NaN\n",
    "\n",
    "def mean_bytes_window(row, window, f_or_b, num):\n",
    "    if len(window) == num:\n",
    "        window.pop(0)\n",
    "    window.append(row)\n",
    "    arr = src_windows(window, f_or_b)\n",
    "    return arr.mean() if len(arr) != 0 else np.NaN\n",
    "\n",
    "def std_bytes_window(row, window, f_or_b, num):\n",
    "    if len(window) == num:\n",
    "        window.pop(0)\n",
    "    window.append(row)\n",
    "    arr = src_windows(window, f_or_b)\n",
    "    return arr.std() if len(arr) != 0 else np.NaN\n",
    "\n",
    "for frames in split_frames:\n",
    "    forward_df = frames[['SrcAddr', 'totbytes_f', 'is_forward']].copy()\n",
    "    backward_df = frames[['SrcAddr', 'totbytes_b', 'is_backward']].copy()\n",
    "\n",
    "    queue = []\n",
    "    frames['sum_size_f_num_5'] = [sum_bytes_window(row, queue, 'is_forward', 5) for index, row in forward_df.iterrows()]\n",
    "    queue = []\n",
    "    frames['sum_size_b_num_5'] = [sum_bytes_window(row, queue, 'is_backward', 5) for index, row in backward_df.iterrows()]\n",
    "\n",
    "    queue = []\n",
    "    frames['min_size_f_num_5'] = [min_bytes_window(row, queue, 'is_forward', 5) for index, row in forward_df.iterrows()]\n",
    "    queue = []\n",
    "    frames['min_size_b_num_5'] = [min_bytes_window(row, queue, 'is_backward', 5) for index, row in backward_df.iterrows()]\n",
    "\n",
    "    queue = []\n",
    "    frames['max_size_f_num_5'] = [max_bytes_window(row, queue, 'is_forward', 5) for index, row in forward_df.iterrows()]\n",
    "    queue = []\n",
    "    frames['max_size_b_num_5'] = [max_bytes_window(row, queue, 'is_backward', 5) for index, row in backward_df.iterrows()]\n",
    "\n",
    "    queue = []\n",
    "    frames['mean_size_f_num_5'] = [mean_bytes_window(row, queue, 'is_forward', 5) for index, row in forward_df.iterrows()]\n",
    "    queue = []\n",
    "    frames['mean_size_b_num_5'] = [mean_bytes_window(row, queue, 'is_backward', 5) for index, row in backward_df.iterrows()]\n",
    "\n",
    "    queue = []\n",
    "    frames['std_size_f_num_5'] = [std_bytes_window(row, queue, 'is_forward', 5) for index, row in forward_df.iterrows()]\n",
    "    queue = []\n",
    "    frames['std_size_b_num_5'] = [std_bytes_window(row, queue, 'is_backward', 5) for index, row in backward_df.iterrows()]\n",
    "\n",
    "    queue = []\n",
    "    frames['total_flow_f_num_5'] = [sum_flow_window(row, queue, 'is_forward', 5) for index, row in forward_df.iterrows()]\n",
    "    queue = []\n",
    "    frames['total_flow_b_num_5'] = [sum_flow_window(row, queue, 'is_backward', 5) for index, row in backward_df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_df[['TotBytes', \n",
    "#             'min_size_f_time_10', \n",
    "#             'min_size_b_time_10', \n",
    "#             'max_size_f_time_10',\n",
    "#             'max_size_b_time_10',\n",
    "#             'mean_size_f_time_10',\n",
    "#             'mean_size_b_time_10',\n",
    "#             'std_size_f_time_10',\n",
    "#             'std_size_b_time_10',\n",
    "#             'epoch']].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine all frames\n",
    "final_df = split_frames[0].copy()\n",
    "for i in range(1, len(split_frames)):\n",
    "    final_df = final_df.append(split_framesp[i], ignore_index=True)\n",
    "\n",
    "#Write to csv file.\n",
    "final_df.to_csv('../feature.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('RDM-env': venv)",
   "language": "python",
   "name": "python38164bitrdmenvvenv4b617ac6a3c04225a7e20584d0a1c7e8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
