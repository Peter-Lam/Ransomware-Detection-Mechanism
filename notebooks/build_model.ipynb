{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import makedirs\n",
    "from os.path import dirname\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import preprocessing\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.model_selection import KFold\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from scipy.stats import uniform, expon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = pd.read_csv('../data/processed/processed.csv')\n",
    "feature_df.loc[feature_df.Label == 0, 'Label'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_x = feature_df.drop(columns=col_exclude_training).to_numpy()\n",
    "# feature_y = feature_df['Label'].to_numpy()\n",
    "\n",
    "malicious_df = feature_df.loc[feature_df['Label'] == 1]\n",
    "# malicious_x = malicious_df.drop(columns=cols_not_in_training).to_numpy()\n",
    "# malicious_y = malicious_df['Label'].to_numpy()\n",
    "\n",
    "benign_df = feature_df.loc[feature_df['Label'] == -1]\n",
    "# benign_x = benign_df.drop(columns=cols_not_in_training).to_numpy()\n",
    "# benign_y  = benign_df['Label'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(malicious_df, malicious_df['Label'], test_size=0.2, random_state=42)\n",
    "malicious_df = None\n",
    "# df[df.columns.difference(col_exclude_training)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.concat([feature_df.loc[feature_df['Label'] == 1].sample(100, random_state=2),\n",
    "#                   feature_df.loc[feature_df['Label'] == -1].sample(100, random_state=2)],\n",
    "#                  axis=0)\n",
    "# X = test.drop(columns=cols_not_in_training).to_numpy()\n",
    "# Y = test['Label'].to_numpy()\n",
    "# X = feature_x\n",
    "# Y = feature_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_calibrated_clf(clf):\n",
    "    # clf_2.predict_proba(scaler_2.transform(X))\n",
    "    return CalibratedClassifierCV(clf)\n",
    "\n",
    "def get_confusion_matrix(true_label, predict_results):\n",
    "    #tn, fp, fn, tp = confusion_matrix(true_label, predict_results).ravel()\n",
    "    return confusion_matrix(true_label, predict_results).ravel()\n",
    "\n",
    "def df_to_nump(df):\n",
    "    col_exclude_training = ['StartTime', 'Dir', 'Proto', 'State', 'Label', 'SrcAddr', 'Sport', 'DstAddr', 'Dport', 'sTos', 'dTos', 'is_fwd' ]\n",
    "    return df.drop(columns=col_exclude_training).to_numpy()\n",
    "\n",
    "def df_to_labels(df):\n",
    "    return df['Label'].to_numpy()\n",
    "\n",
    "def fit_predict_model(clf, X, y, scaler_obj):\n",
    "    print('Training Model')\n",
    "    scaled = scaler_obj.fit(X)\n",
    "    x_scaled = scaled.transform(X)\n",
    "    self_predict_r = clf.fit_predict(x_scaled, y=y)\n",
    "    print('Training Model Completed')\n",
    "    return {'model': clf, 'self_predict': self_predict_r}\n",
    "\n",
    "def save_model(model, dir_path, model_name):\n",
    "    print('Saving Model')\n",
    "    pickle.dump(model, open(f'{dir_path}{model_name}.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring=['accuracy', 'f1', 'recall', 'precision', 'roc_auc']\n",
    "# roc_auc\n",
    "#Precision = False Positives, at first should be no false positives\n",
    "#Recall = False Negativives\n",
    "# f1 =  2 * (precision * recall)/ (precision + recall)\n",
    "clf_list = []\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "# clf_list.append(make_pipeline(preprocessing.StandardScaler(), LinearSVC(C=27.534917537749216, dual=False, tol=0.0048028537307841352)))\n",
    "# clf_list.append(make_pipeline(preprocessing.StandardScaler(), OneClassSVM(kernel=\"rbf\", gamma=0.0121072443425558, cache_size=500, nu=0.11932807423095282)))\n",
    "\n",
    "# clf_list.append(svm.OneClassSVM(kernel=\"rbf\", gamma=1e-05, cache_size=400, nu=1e-05))\n",
    "clf_list.append(make_pipeline(preprocessing.StandardScaler(), OneClassSVM(kernel=\"rbf\", gamma=1e-05, cache_size=500, nu=1e-05)))\n",
    "# clf_list.append(make_pipeline(preprocessing.StandardScaler(), svm.OneClassSVM(kernel=\"rbf\", gamma=1e-02, cache_size=1000, nu=1e-05)))\n",
    "# clf_list.append(make_pipeline(preprocessing.RobustScaler(), svm.OneClassSVM(kernel=\"rbf\", gamma=1e-05, cache_size=1000, nu=1e-05)))\n",
    "# clf_list.append(make_pipeline(preprocessing.RobustScaler(), svm.OneClassSVM(kernel=\"rbf\", gamma='scale', cache_size=1000, nu=1e-05)))\n",
    "# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "fold = 1\n",
    "for clf in clf_list:\n",
    "    scores = cross_validate(clf, X, Y, scoring='accuracy', cv=skf, n_jobs=5)\n",
    "    print(scores.keys())\n",
    "    count = 0\n",
    "    print(f'----Classifier #{fold}-----')\n",
    "    print(scores['test_score'])\n",
    "    fold = fold + 1\n",
    "    print(\"Sum Fit Time: %0.5f\" % (scores['fit_time'].sum()))\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores['test_accuracy'].mean()*100, scores['test_accuracy'].std() * 2))\n",
    "    print(\"Recall: %0.2f (+/- %0.2f)\" % (scores['test_recall'].mean()*100, scores['test_recall'].std() * 2))\n",
    "    print(\"F1: %0.2f (+/- %0.2f)\" % (scores['test_f1'].mean()*100, scores['test_f1'].std() * 2))\n",
    "    print(\"Precision: %0.2f (+/- %0.2f)\" % (scores['test_precision'].mean()*100, scores['test_precision'].std() * 2))\n",
    "    print(\"ROC: %0.2f (+/- %0.2f)\" % (scores['test_roc_auc'].mean()*100, scores['test_roc_auc'].std() * 2))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_tuning(classifier, tuned_parameters, X, y):\n",
    "#     score = 'roc_auc'\n",
    "    score = 'f1'\n",
    "#     score = 'accuracy'\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        classifier, tuned_parameters, scoring=score, n_jobs=5, cv=3, verbose=25\n",
    "    )\n",
    "    scaler = preprocessing.StandardScaler().fit(X)\n",
    "    clf.fit(scaler.transform(X), y)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "    return clf.best_params_\n",
    "\n",
    "def tune_linear_svc(X, y):\n",
    "    tuned_parameters = [{'tol': expon(scale=.01),\n",
    "                     'C': expon(scale=100),\n",
    "                     'dual': [False]\n",
    "                    }]\n",
    "    hyper_tuning(LinearSVC(), tuned_parameters, X, y)\n",
    "    \n",
    "    # 0.849 (+/-0.015) for {'C': 27.534917537749216, 'dual': False, 'tol': 0.004802853730784135}\n",
    "    # {'C': 34.49337686017465, 'dual': False, 'tol': 0.0004567829261173656} sample = 100000, F1.\n",
    "    # 0.872 (+/-0.006) for {'C': 10.348501146284026, 'dual': False, 'tol': 0.00017506509292527104} Sample = 100k. Precision\n",
    "    # 0.949 {'C': 32.560339433948236, 'dual': False, 'tol': 0.0014118363259887406} 100k, roc_auc\n",
    "    #0.952 (+/-0.002) for {'C': 194.35726726323622, 'dual': False, 'tol': 0.003035578173309056} Sample = 200k. average precision\n",
    "def tune_oneclass(X, y, expected_outliar_size):\n",
    "    #     tuned_parameters = [{'kernel': ['rbf'],\n",
    "    #                          'gamma': expon(scale=.1),\n",
    "    #                          'nu': expon(scale=0.1)}]\n",
    "    nu =  expected_outliar_size/len(X)\n",
    "    tuned_parameters = [{'kernel': ['rbf'],\n",
    "                         'gamma': [1e-7, 1e-6, 1e-5, 1e-4],\n",
    "                         'nu': [nu]}]\n",
    "    return hyper_tuning(OneClassSVM(), tuned_parameters, X, y)\n",
    "    # 0.500 (+/-0.846) for {'gamma': 0.0121072443425558, 'kernel': 'rbf', 'nu': 0.11932807423095282}\n",
    "    # 0.351 (+/-0.589) for {'gamma': 0.5389663750979422, 'kernel': 'rbf', 'nu': 0.03258700301586109}\n",
    "    # 0.477 (+/-0.807) for {'gamma': 0.03226814043225676, 'kernel': 'rbf', 'nu': 0.15264728096179458}\n",
    "    # 0.490 (+/-0.829) for {'gamma': 0.03549584357487325, 'kernel': 'rbf', 'nu': 0.11916614740608962}\n",
    "    # 0.486 (+/-0.822) for {'gamma': 0.04714615631138286, 'kernel': 'rbf', 'nu': 0.1208709207242509}\n",
    "    # 0.948 (+/-0.003) for {'gamma': 1e-07, 'kernel': 'rbf', 'nu': 0.1} 50k\n",
    "    # 0.948 (+/-0.001) for {'gamma': 1e-07, 'kernel': 'rbf', 'nu': 0.1} 30k F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_performance_metrics(y_true, y_pred):\n",
    "    metric_results_dict = {}\n",
    "    metric_results_dict['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    metric_results_dict['recall'] = recall_score(y_true, y_pred, average='binary')\n",
    "    metric_results_dict['precision'] = precision_score(y_true, y_pred, average='binary')\n",
    "    metric_results_dict['f1'] = f1_score(y_true, y_pred, average='binary')\n",
    "    metric_results_dict['average_precision'] = average_precision_score(y_true, y_pred)\n",
    "    metric_results_dict['confusion_matrix'] = get_confusion_matrix(y_true, y_pred)\n",
    "    return metric_results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test, y_test)\n",
    "    print(model_performance_metrics(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   1 tasks      | elapsed:   34.0s\n",
      "[Parallel(n_jobs=5)]: Done   2 tasks      | elapsed:   34.1s\n",
      "[Parallel(n_jobs=5)]: Done   3 tasks      | elapsed:   34.4s\n",
      "[Parallel(n_jobs=5)]: Done   4 tasks      | elapsed:   39.2s\n",
      "[Parallel(n_jobs=5)]: Done   5 tasks      | elapsed:   39.5s\n",
      "[Parallel(n_jobs=5)]: Done   6 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=5)]: Done   7 out of  15 | elapsed:  1.2min remaining:  1.4min\n",
      "[Parallel(n_jobs=5)]: Done   8 out of  15 | elapsed:  1.2min remaining:  1.1min\n",
      "[Parallel(n_jobs=5)]: Done   9 out of  15 | elapsed:  1.3min remaining:   52.3s\n",
      "[Parallel(n_jobs=5)]: Done  10 out of  15 | elapsed:  1.3min remaining:   39.5s\n",
      "[Parallel(n_jobs=5)]: Done  11 out of  15 | elapsed:  1.9min remaining:   41.2s\n",
      "[Parallel(n_jobs=5)]: Done  12 out of  15 | elapsed:  1.9min remaining:   28.3s\n",
      "[Parallel(n_jobs=5)]: Done  13 out of  15 | elapsed:  1.9min remaining:   17.5s\n",
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed:  2.0min remaining:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'gamma': 1e-07, 'kernel': 'rbf', 'nu': 0.1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.851 (+/-0.129) for {'gamma': 1e-08, 'kernel': 'rbf', 'nu': 0.1}\n",
      "0.948 (+/-0.001) for {'gamma': 1e-07, 'kernel': 'rbf', 'nu': 0.1}\n",
      "0.947 (+/-0.001) for {'gamma': 1e-06, 'kernel': 'rbf', 'nu': 0.1}\n",
      "0.947 (+/-0.001) for {'gamma': 1e-05, 'kernel': 'rbf', 'nu': 0.1}\n",
      "0.947 (+/-0.003) for {'gamma': 0.0001, 'kernel': 'rbf', 'nu': 0.1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# best_params = tune_oneclass(df_to_nump(X_train[:30000]), y_train[:30000], .10*30000)\n",
    "# len(benign_df)\n",
    "# clf = OneClassSVM(kernel=best_params['kernel'], nu=best_params['nu'], gamma=best_params['gamma'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model\n",
      "[LibSVM]Training Model Completed\n",
      "Saving Model\n",
      "{'accuracy': 0.8201183818467769, 'recall': 0.8201183818467769, 'precision': 1.0, 'f1': 0.9011703744397621, 'average_precision': 1.0, 'confusion_matrix': array([     0,      0, 143198, 652870], dtype=int64)}\n",
      "\n",
      "Predicted      -1       1     All\n",
      "Actual                           \n",
      "1          143198  652870  796068\n",
      "All        143198  652870  796068\n",
      "\n",
      "Predicted        -1         1\n",
      "Actual                       \n",
      "1          0.179882  0.820118\n",
      "\n",
      "{'accuracy': 0.7289893313930553, 'recall': 0.8208704740274749, 'precision': 0.7410223030621919, 'f1': 0.7789053616254372, 'average_precision': 0.7124563879518993, 'confusion_matrix': array([ 86106,  57095,  35650, 163368], dtype=int64)}\n",
      "\n",
      "Predicted      -1       1     All\n",
      "Actual                           \n",
      "-1          86106   57095  143201\n",
      "1           35650  163368  199018\n",
      "All        121756  220463  342219\n",
      "\n",
      "Predicted        -1         1\n",
      "Actual                       \n",
      "-1         0.601295  0.398705\n",
      " 1         0.179130  0.820870\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nu =  len(benign_df)/len(X_train)\n",
    "kernel= 'rbf'\n",
    "gamma= 1e-7\n",
    "clf = OneClassSVM(kernel=kernel, nu=nu, gamma=gamma, cache_size=10000, verbose=True)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "modeling_dict = fit_predict_model(clf, df_to_nump(X_train), y_train, scaler)\n",
    "clf = modeling_dict['model']\n",
    "save_model(clf, '../models/', 'oneclass')\n",
    "self_predict_r = modeling_dict['self_predict']\n",
    "print(model_performance_metrics(y_train, self_predict_r))\n",
    "print()\n",
    "df_confusion_train = pd.crosstab(y_train, self_predict_r, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "df_confusion_train_norm = pd.crosstab(y_train, self_predict_r, rownames=['Actual'], colnames=['Predicted'], normalize='index')\n",
    "print(df_confusion_train)\n",
    "print()\n",
    "print(df_confusion_train_norm)\n",
    "print()\n",
    "y_test_final = np.concatenate((y_test, (df_to_labels(benign_df))))\n",
    "X_test_final = np.concatenate((df_to_nump(X_test), (df_to_nump(benign_df))))\n",
    "testing_results = clf.predict(scaler.transform(X_test_final))\n",
    "print(model_performance_metrics(y_test_final, testing_results))\n",
    "print()\n",
    "df_confusion_test = pd.crosstab(y_test_final, testing_results, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "df_confusion_test_norm = pd.crosstab(y_test_final, testing_results, rownames=['Actual'], colnames=['Predicted'], normalize='index')\n",
    "print(df_confusion_test)\n",
    "print()\n",
    "print(df_confusion_test_norm)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_benign_df = pd.concat([X_test, benign_df])\n",
    "X_test = None\n",
    "benign_df = None\n",
    "x_test_benign_label = x_test_benign_df['Label']\n",
    "x_test_benign_df = x_test_benign_df.drop(columns=['Label'])\n",
    "x_test_benign_df['Label'] = x_test_benign_label\n",
    "x_test_benign_df['Predicted Label'] = testing_results\n",
    "\n",
    "mal_train_df = X_train.copy()\n",
    "X_train = None\n",
    "mal_train_label = mal_train_df['Label']\n",
    "mal_train_df = mal_train_df.drop(columns=['Label'])\n",
    "mal_train_df['Label'] = mal_train_label\n",
    "mal_train_df['Predicted Label'] = self_predict_r\n",
    "\n",
    "final_df = pd.concat([x_test_benign_df, mal_train_df])\n",
    "final_df = final_df.sort_values('StartTime', ignore_index=True)\n",
    "makedirs(dirname('../data/trained/'), exist_ok=True)\n",
    "final_df.to_csv('../data/trained/trained.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1\n",
    "# 0.818 (+/-0.237) for {'gamma': 1e-08, 'kernel': 'rbf', 'nu': 0.1}\n",
    "# 0.946 (+/-0.004) for {'gamma': 1e-07, 'kernel': 'rbf', 'nu': 0.1}\n",
    "# 0.947 (+/-0.003) for {'gamma': 1e-06, 'kernel': 'rbf', 'nu': 0.1}\n",
    "# 0.947 (+/-0.002) for {'gamma': 1e-05, 'kernel': 'rbf', 'nu': 0.1}\n",
    "# 0.947 (+/-0.002) for {'gamma': 0.0001, 'kernel': 'rbf', 'nu': 0.1}\n",
    "\n",
    "# Accuracy\n",
    "# 0.709 (+/-0.344) for {'gamma': 1e-08, 'kernel': 'rbf', 'nu': 0.1}\n",
    "# 0.898 (+/-0.007) for {'gamma': 1e-07, 'kernel': 'rbf', 'nu': 0.1}\n",
    "# 0.900 (+/-0.005) for {'gamma': 1e-06, 'kernel': 'rbf', 'nu': 0.1}\n",
    "# 0.900 (+/-0.004) for {'gamma': 1e-05, 'kernel': 'rbf', 'nu': 0.1}\n",
    "# 0.899 (+/-0.003) for {'gamma': 0.0001, 'kernel': 'rbf', 'nu': 0.1}\n",
    "\n",
    "# F1 75k\n",
    "# {'gamma': 1e-06, 'kernel': 'rbf', 'nu': 0.1}\n",
    "\n",
    "# Grid scores on development set:\n",
    "\n",
    "# 0.946 (+/-0.007) for {'gamma': 1e-07, 'kernel': 'rbf', 'nu': 0.1}\n",
    "# 0.947 (+/-0.004) for {'gamma': 1e-06, 'kernel': 'rbf', 'nu': 0.1}\n",
    "# 0.947 (+/-0.004) for {'gamma': 1e-05, 'kernel': 'rbf', 'nu': 0.1}\n",
    "# 0.947 (+/-0.004) for {'gamma': 0.001, 'kernel': 'rbf', 'nu': 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Linear Regression\n",
    "# 2. Logistic Regression\n",
    "# 3. CART\n",
    "# 4. Na√Øve Bayes\n",
    "# 5. KNN\n",
    "# 6. Random Forests\n",
    "# Logistic Regression, Support Vector Machine, Decision Tree, Random Forest, and Adaboosting\n",
    "# RandomForest 1 0 1 1\n",
    "# AdaBoostM1 1 0 1 1\n",
    "# Bagging 1 0 1 1\n",
    "# LogitBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import OneClassSVM \n",
    "# train, test = train_test_split(data, test_size=.2) \n",
    "# train_normal = train[train['y']==0] \n",
    "# train_outliers = train[train['y']==1] \n",
    "# outlier_prop = len(train_outliers) / len(train_normal) \n",
    "# svm = OneClassSVM(kernel='rbf', nu=outlier_prop, gamma=0.000001) svm.fit(train_normal[['x1','x4','x5']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_minority_upsampled = resample(df_minority, \n",
    "#                                  replace=True,     # sample with replacement\n",
    "#                                  n_samples=576,    # to match majority class\n",
    "#                                  random_state=123) # reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_predict(X_train, X_test):\n",
    "#     clf = svm.OneClassSVM(kernel=\"rbf\", gamma='scale', cache_size=8000, nu=0.01)\n",
    "#     scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "# #     scaler = preprocessing.RobustScaler().fit(X_train)\n",
    "#     print(f'Training: {get_percentage(clf.fit_predict(scaler.transform(X_train)))}%')\n",
    "#     benign_test = benign_df.to_numpy()\n",
    "#     real_acc_results = clf.predict(scaler.transform(benign_test))\n",
    "# #     real_df = pd.DataFrame(data={'Results': real_acc_results})\n",
    "# #     print(real_df['Results'].value_counts())\n",
    "# #     print(real_df.head(10))\n",
    "#     print(f'Real Accuracy: {get_percentage(real_acc_results, False)}%' )\n",
    "#     test_result = clf.predict(scaler.transform(X_test))\n",
    "#     return test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_decision(X_train, X_test):\n",
    "#     clf = svm.OneClassSVM(kernel=\"rbf\", gamma='scale', cache_size=5000)\n",
    "#     scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "# #     scaler = preprocessing.RobustScaler().fit(X_train)\n",
    "#     clf.fit(scaler.transform(X_train))\n",
    "#     return np.sum(clf.score_samples(scaler.transform(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_percentage(arr, is_test=True):\n",
    "#     total = len(arr)\n",
    "#     count = 0\n",
    "#     if is_test:\n",
    "#         for item in arr:\n",
    "#             if item == 1:\n",
    "#                 count = count + 1\n",
    "#     else:\n",
    "#         for item in arr:\n",
    "#             if item == -1:\n",
    "#                 count = count + 1\n",
    "#     return (count/total * 100)\n",
    "#     print(f'Percentage: {(count/total * 100)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benign_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = feature_df.iloc[:200000].to_numpy()\n",
    "# kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n",
    "\n",
    "# add = 0\n",
    "# score = []\n",
    "# for train_index, test_index in kf.split(X):\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     start = time.time()\n",
    "#     result = get_predict(X_train, X_test)\n",
    "#     score.append(result)\n",
    "# for single_test in score:\n",
    "#     print(f'Test: {get_percentage(single_test)}%')\n",
    "\n",
    "# for s in score:\n",
    "#     add = add + get_percentage(s)\n",
    "# mean = add/len(score)\n",
    "# print(f'Mean Test: {mean}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = []\n",
    "# for train_index, test_index in kf.split(X):\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     results.append(get_predict(X_train, X_test))\n",
    "#     print('One Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_percentage(clf_2.predict(scaler_2.transform(benign_df.to_numpy())), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_percentage(clf_2.predict(scaler_2.transform(final_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = feature_x[:150000]\n",
    "# Y = feature_y[:150000]\n",
    "\n",
    "# kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "# for train_index, test_index in kf.split(X, Y):\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = Y[train_index], Y[test_index]\n",
    "#     clf_2 = svm.OneClassSVM(kernel=\"rbf\", gamma='scale', cache_size=400, nu=0.01)\n",
    "#     scaler_2 = preprocessing.StandardScaler().fit(X_train)\n",
    "#     clf_2.fit(scaler_2.transform(X_train))\n",
    "#     result = clf_2.predict(scaler_2.transform(X_test))\n",
    "#     print((len([ res for res in result if res == 1])/len(result))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy, precicion , recall, confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw+ discretized + engineered + real label + predicted label + confidence score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('RDM-env': venv)",
   "language": "python",
   "name": "python38164bitrdmenvvenv4b617ac6a3c04225a7e20584d0a1c7e8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
